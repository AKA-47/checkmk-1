#!/usr/bin/env python
# -*- encoding: utf-8; py-indent-offset: 4 -*-
# +------------------------------------------------------------------+
# |             ____ _               _        __  __ _  __           |
# |            / ___| |__   ___  ___| | __   |  \/  | |/ /           |
# |           | |   | '_ \ / _ \/ __| |/ /   | |\/| | ' /            |
# |           | |___| | | |  __/ (__|   <    | |  | | . \            |
# |            \____|_| |_|\___|\___|_|\_\___|_|  |_|_|\_\           |
# |                                                                  |
# | Copyright Mathias Kettner 2018             mk@mathias-kettner.de |
# +------------------------------------------------------------------+
#
# This file is part of Check_MK.
# The official homepage is at http://mathias-kettner.de/check_mk.
#
# check_mk is free software;  you can redistribute it and/or modify it
# under the  terms of the  GNU General Public License  as published by
# the Free Software Foundation in version 2.  check_mk is  distributed
# in the hope that it will be useful, but WITHOUT ANY WARRANTY;  with-
# out even the implied warranty of  MERCHANTABILITY  or  FITNESS FOR A
# PARTICULAR PURPOSE. See the  GNU General Public License for more de-
# tails. You should have  received  a copy of the  GNU  General Public
# License along with GNU Make; see the file  COPYING.  If  not,  write
# to the Free Software Foundation, Inc., 51 Franklin St,  Fifth Floor,
# Boston, MA 02110-1301 USA.
"""
Special agent for monitoring Amazon web services (AWS) with Check_MK.
"""

import abc
import argparse
import datetime
import json
import logging
import sys
import time
import errno
from typing import NamedTuple, Any, List
from pathlib2 import Path
import boto3  # type: ignore
import botocore  # type: ignore
from cmk.utils.paths import tmp_dir
import cmk.utils.store as store
import cmk.utils.password_store

#   .--helpers-------------------------------------------------------------.
#   |                  _          _                                        |
#   |                 | |__   ___| |_ __   ___ _ __ ___                    |
#   |                 | '_ \ / _ \ | '_ \ / _ \ '__/ __|                   |
#   |                 | | | |  __/ | |_) |  __/ |  \__ \                   |
#   |                 |_| |_|\___|_| .__/ \___|_|  |___/                   |
#   |                              |_|                                     |
#   '----------------------------------------------------------------------'


def _datetime_converter(o):
    if isinstance(o, datetime.datetime):
        return o.__str__()


def _chunks(list_, length=100):
    return [list_[i:i + length] for i in xrange(0, len(list_), length)]


#.
#   .--section API---------------------------------------------------------.
#   |                       _   _                  _    ____ ___           |
#   |         ___  ___  ___| |_(_) ___  _ __      / \  |  _ \_ _|          |
#   |        / __|/ _ \/ __| __| |/ _ \| '_ \    / _ \ | |_) | |           |
#   |        \__ \  __/ (__| |_| | (_) | | | |  / ___ \|  __/| |           |
#   |        |___/\___|\___|\__|_|\___/|_| |_| /_/   \_\_|  |___|          |
#   |                                                                      |
#   '----------------------------------------------------------------------'


class AWSSectionError(Exception):
    pass


#   ---result distributor---------------------------------------------------


class ResultDistributor(object):
    """
    Mediator which distributes results from sections
    in order to reduce queries to AWS account.
    """

    def __init__(self):
        self._colleagues = []

    def add(self, colleague):
        self._colleagues.append(colleague)

    def distribute(self, sender, result):
        for colleague in self._colleagues:
            if colleague.name != sender.name:
                colleague.receive(sender, result)


#   ---sections/colleagues--------------------------------------------------

AWSSectionResults = NamedTuple("AWSSectionResults", [
    ("results", List),
    ("cache_timestamp", int),
])

AWSSectionResult = NamedTuple("AWSSectionResult", [
    ("piggyback_hostname", str),
    ("content", Any),
])

AWSColleagueContents = NamedTuple("AWSColleagueContents", [
    ("content", Any),
    ("cache_timestamp", float),
])

AWSRawContent = NamedTuple("AWSRawContent", [
    ("content", Any),
    ("cache_timestamp", float),
])

AWSComputedContent = NamedTuple("AWSComputedContent", [
    ("content", Any),
    ("cache_timestamp", float),
])

AWSCacheFilePath = Path(tmp_dir) / "agents" / "agent_aws"


class AWSSection(object):
    __metaclass__ = abc.ABCMeta

    def __init__(self, client, hostname, region, distributor=None):
        self._client = client
        self._hostname = hostname
        self._region = region
        self._distributor = ResultDistributor() if distributor is None else distributor
        self._received_results = {}
        self._cache_file_dir = AWSCacheFilePath / region / hostname
        self._cache_file = AWSCacheFilePath / region / hostname / self.name

    @abc.abstractproperty
    def name(self):
        pass

    @abc.abstractproperty
    def interval(self):
        """
        In general the default resolution of AWS metrics is 5 min (300 sec)
        The default resolution of AWS S3 metrics is 1 day (86400 sec)
        We use interval property for cached section.
        """
        pass

    @property
    def period(self):
        return 2 * self.interval

    def _send(self, content):
        self._distributor.distribute(self, content)

    def receive(self, sender, content):
        self._received_results.setdefault(sender.name, content)

    def run(self, use_cache=False):
        colleague_contents = self._get_colleague_contents()
        assert isinstance(colleague_contents, AWSColleagueContents)

        raw_content = self._get_raw_content(colleague_contents, use_cache=use_cache)
        assert isinstance(raw_content, AWSRawContent)

        computed_content = self._compute_content(raw_content, colleague_contents)
        assert isinstance(computed_content, AWSComputedContent)

        self._send(computed_content)
        return AWSSectionResults(
            self._create_results(computed_content), computed_content.cache_timestamp)

    def _get_raw_content(self, colleague_contents, use_cache=False):
        # Cache is only used if the age is lower than section interval AND
        # the collected data from colleagues are not newer
        self._cache_file_dir.mkdir(parents=True, exist_ok=True)
        if use_cache and self._cache_is_recent_enough(colleague_contents):
            raw_content, cache_timestamp = self._read_from_cache()
        else:
            raw_content = self._fetch_raw_content(colleague_contents)
            # TODO: Write cache only when _compute_section_content succeeded?
            self._write_to_cache(raw_content)
            cache_timestamp = time.time()
        return AWSRawContent(raw_content, cache_timestamp)

    def _cache_is_recent_enough(self, colleague_contents):
        if not self._cache_file.exists():
            logging.info("New cache file %s", self._cache_file)
            return False

        now = time.time()
        try:
            age = now - self._cache_file.stat().st_mtime
        except OSError as e:
            if e.errno == 2:  # No such file or directory
                logging.info("Cannot calculate cache file age of %s", self._cache_file)
                return False
            else:
                raise

        if age >= self.interval:
            logging.info("Cache file %s is outdated", self._cache_file)
            return False

        if colleague_contents.cache_timestamp > now:
            logging.info("Colleague data is newer than cache file %s", self._cache_file)
            return False
        return True

    def _read_from_cache(self):
        try:
            with self._cache_file.open(encoding="utf-8") as f:
                raw_content = f.read().strip()
        except IOError as e:
            if e.errno == errno.ENOENT:  # No such file or directory
                return None, time.time()
            else:
                raise
        try:
            content = json.loads(raw_content)
        except ValueError as e:
            logging.debug(e)
            content = None
        return content, self._cache_file.stat().st_mtime

    def _write_to_cache(self, raw_content):
        json_dump = json.dumps(raw_content, default=_datetime_converter)
        store.save_file(str(self._cache_file), json_dump)

    @abc.abstractmethod
    def _get_colleague_contents(self):
        # type: AWSColleagueContents
        """
        Receive section contents from colleagues. The results are stored in
        self._receive_results: {<KEY>: AWSComputedContent}.
        The relation between two sections must be declared in the related
        distributor in advance to make this work.
        Use max. cache_timestamp of all received results for
        AWSColleagueContents.cache_timestamp
        """
        pass

    @abc.abstractmethod
    def _fetch_raw_content(self, colleague_contents):
        """
        Call API methods, eg. 'response = ec2_client.describe_instances()' and
        extract content from raw content.  Raw contents basically consist of
        two sub results:
        - 'ResponseMetadata'
        - '<KEY>'
        Return raw_result['<KEY>'].
        """
        pass

    @abc.abstractmethod
    def _compute_content(self, raw_content, colleague_contents):
        # type: (AWSRawContent, Any) -> AWSComputedContent
        """
        Compute the final content of this section based on the raw content of
        this section and the content received from the optional colleague
        sections.
        """
        pass

    @abc.abstractmethod
    def _create_results(self, computed_content):
        # type: (Any) -> List[AWSSectionResult]
        pass


class AWSSectionGeneric(AWSSection):
    __metaclass__ = abc.ABCMeta


class AWSSectionCloudwatch(AWSSection):
    __metaclass__ = abc.ABCMeta

    def _fetch_raw_content(self, colleague_contents):
        end_time = time.time()
        start_time = end_time - self.period
        metrics = self._get_metrics(colleague_contents)
        if not metrics:
            return []

        # A single GetMetricData call can include up to 100 MetricDataQuery structures
        # There's no pagination for this operation:
        # self._client.can_paginate('get_metric_data') = False
        raw_content = []
        for chunk in _chunks(metrics):
            if not chunk:
                continue
            response = self._client.get_metric_data(
                MetricDataQueries=chunk,
                StartTime=start_time,
                EndTime=end_time,
            )
            try:
                metrics = response['MetricDataResults']
            except KeyError as e:
                logging.info("%s: KeyError %s; Available are %s", self.name, e, response.keys())
                continue
            raw_content.extend(metrics)
        return raw_content

    @abc.abstractmethod
    def _get_metrics(self, colleague_contents):
        pass


#.
#   .--costs/usage---------------------------------------------------------.
#   |                      _          __                                   |
#   |         ___ ___  ___| |_ ___   / /   _ ___  __ _  __ _  ___          |
#   |        / __/ _ \/ __| __/ __| / / | | / __|/ _` |/ _` |/ _ \         |
#   |       | (_| (_) \__ \ |_\__ \/ /| |_| \__ \ (_| | (_| |  __/         |
#   |        \___\___/|___/\__|___/_/  \__,_|___/\__,_|\__, |\___|         |
#   |                                                  |___/               |
#   '----------------------------------------------------------------------'

# Interval between 'Start' and 'End' must be a DateInterval. 'End' is exclusive.
# Example:
# 2017-01-01 - 2017-05-01; cost and usage data is retrieved from 2017-01-01 up
# to and including 2017-04-30 but not including 2017-05-01.
# The GetCostAndUsageRequest operation supports only DAILY and MONTHLY granularities.


class CostsAndUsage(AWSSectionGeneric):
    @property
    def name(self):
        return "costs_and_usage"

    @property
    def interval(self):
        return 86400

    def _get_colleague_contents(self):
        return AWSColleagueContents(None, 0)

    def _fetch_raw_content(self, colleague_contents):
        fmt = "%Y-%m-%d"
        now = time.time()
        response = self._client.get_cost_and_usage(
            TimePeriod={
                'Start': time.strftime(fmt, time.gmtime(now - self.interval)),
                'End': time.strftime(fmt, time.gmtime(now)),
            },
            Granularity='DAILY',
            Metrics=['UnblendedCost'],
            GroupBy=[{
                'Type': 'DIMENSION',
                'Key': 'LINKED_ACCOUNT'
            }, {
                'Type': 'DIMENSION',
                'Key': 'SERVICE'
            }],
        )
        try:
            return response['ResultsByTime']
        except KeyError as e:
            logging.info("%s: KeyError %s; Available are %s", self.name, e, response.keys())
            return []

    def _compute_content(self, raw_content, colleague_contents):
        return AWSComputedContent(raw_content.content, raw_content.cache_timestamp)

    def _create_results(self, computed_content):
        return [AWSSectionResult("", computed_content.content)]


#.
#   .--security groups-----------------------------------------------------.
#   |                                         _ _                          |
#   |                ___  ___  ___ _   _ _ __(_) |_ _   _                  |
#   |               / __|/ _ \/ __| | | | '__| | __| | | |                 |
#   |               \__ \  __/ (__| |_| | |  | | |_| |_| |                 |
#   |               |___/\___|\___|\__,_|_|  |_|\__|\__, |                 |
#   |                                               |___/                  |
#   |                                                                      |
#   |                    __ _ _ __ ___  _   _ _ __  ___                    |
#   |                   / _` | '__/ _ \| | | | '_ \/ __|                   |
#   |                  | (_| | | | (_) | |_| | |_) \__ \                   |
#   |                   \__, |_|  \___/ \__,_| .__/|___/                   |
#   |                   |___/                |_|                           |
#   '----------------------------------------------------------------------'


class SecurityGroups(AWSSectionGeneric):
    @property
    def name(self):
        return "security_groups"

    @property
    def interval(self):
        return 300

    def _get_colleague_contents(self):
        return AWSColleagueContents(None, 0)

    def _fetch_raw_content(self, colleague_contents):
        response = self._client.describe_security_groups()
        try:
            return response['SecurityGroups']
        except KeyError as e:
            logging.info("%s: KeyError %s; Available are %s", self.name, e, response.keys())
            return []

    def _compute_content(self, raw_content, colleague_contents):
        return AWSComputedContent(raw_content.content, raw_content.cache_timestamp)

    def _create_results(self, computed_content):
        return [AWSSectionResult("", computed_content.content)]


#.
#   .--EC2-----------------------------------------------------------------.
#   |                          _____ ____ ____                             |
#   |                         | ____/ ___|___ \                            |
#   |                         |  _|| |     __) |                           |
#   |                         | |__| |___ / __/                            |
#   |                         |_____\____|_____|                           |
#   |                                                                      |
#   '----------------------------------------------------------------------'


class EC2Summary(AWSSectionGeneric):
    @property
    def name(self):
        return "ec2_summary"

    @property
    def interval(self):
        return 300

    def _get_colleague_contents(self):
        colleague = self._received_results.get('security_groups')
        if colleague and colleague.content:
            return AWSColleagueContents(colleague.content, colleague.cache_timestamp)
        return AWSColleagueContents([], 0)

    def _fetch_raw_content(self, colleague_contents):
        response = self._client.describe_instances()
        try:
            return response['Reservations']
        except KeyError as e:
            logging.info("%s: KeyError %s; Available are %s", self.name, e, response.keys())
            return {}

    def _compute_content(self, raw_content, colleague_contents):
        reservations = raw_content.content
        for security_group in colleague_contents.content:
            for reservation in reservations:
                if security_group['OwnerId'] == reservation['OwnerId']:
                    reservation['SecurityGroup'] = security_group
        return AWSComputedContent({
            "Reservations": reservations,
            "mapping": self._map_instance_id_to_name(reservations),
        }, raw_content.cache_timestamp)

    def _map_instance_id_to_name(self, reservations):
        mapping = {}
        for reservation in reservations:
            for instance in reservation.get('Instances', []):
                instance_id = instance['InstanceId']
                instance_name = self._extract_instance_name_from_tags(instance, instance_id)
                mapping.setdefault(instance_id, instance_name)
        return mapping

    def _extract_instance_name_from_tags(self, instance, instance_id):
        for tag in instance.get('Tags', []):
            if tag.get('Key') == 'Name':
                return tag['Value']
        return instance_id

    def _create_results(self, computed_content):
        return [AWSSectionResult("", computed_content.content['Reservations'])]


class EC2(AWSSectionCloudwatch):
    @property
    def name(self):
        return "ec2"

    @property
    def interval(self):
        return 300

    def _get_colleague_contents(self):
        colleague = self._received_results.get('ec2_summary')
        if colleague and colleague.content:
            return AWSColleagueContents(
                colleague.content.get('mapping', {}), colleague.cache_timestamp)
        return AWSColleagueContents({}, 0)

    def _get_metrics(self, colleague_contents):
        metrics = []
        idx = 0
        for instance_id, instance_name in colleague_contents.content.iteritems():
            for metric_name, unit in [
                ("CPUCreditUsage", "Count"),
                ("CPUCreditBalance", "Count"),
                ("CPUUtilization", "Percent"),
                ("DiskReadOps", "Count"),
                ("DiskWriteOps", "Count"),
                ("DiskReadBytes", "Bytes"),
                ("DiskWriteBytes", "Bytes"),
                ("NetworkIn", "Bytes"),
                ("NetworkOut", "Bytes"),
                ("StatusCheckFailed_Instance", "Count"),
                ("StatusCheckFailed_System", "Count"),
            ]:
                metrics.append({
                    'Id': "%s_%s" % (metric_name.lower(), idx),
                    'Label': instance_name,
                    'MetricStat': {
                        'Metric': {
                            'Namespace': 'AWS/EC2',
                            'MetricName': metric_name,
                            'Dimensions': [{
                                'Name': "InstanceId",
                                'Value': instance_id,
                            }]
                        },
                        'Period': self.period,
                        'Stat': 'Average',
                        'Unit': unit,
                    },
                })
                idx += 1
        return metrics

    def _compute_content(self, raw_content, colleague_contents):
        content_by_piggyback_hosts = {}
        for row in raw_content.content:
            content_by_piggyback_hosts.setdefault(row['Label'], []).append(row)
        return AWSComputedContent(content_by_piggyback_hosts, raw_content.cache_timestamp)

    def _create_results(self, computed_content):
        return [
            AWSSectionResult(piggyback_hostname, rows)
            for piggyback_hostname, rows in computed_content.content.iteritems()
        ]


#.
#   .--S3------------------------------------------------------------------.
#   |                             ____ _____                               |
#   |                            / ___|___ /                               |
#   |                            \___ \ |_ \                               |
#   |                             ___) |__) |                              |
#   |                            |____/____/                               |
#   |                                                                      |
#   '----------------------------------------------------------------------'


class S3Summary(AWSSectionGeneric):
    def __init__(self, client, hostname, region, distributor=None, buckets=None):
        super(S3Summary, self).__init__(client, hostname, region, distributor=distributor)
        self._buckets = self._determine_buckets(buckets)

    def _determine_buckets(self, buckets):
        if buckets:
            return [{"Name": b} for b in buckets]
        response = self._client.list_buckets()
        try:
            return response['Buckets']
        except KeyError as e:
            logging.info("%s: KeyError %s; Available are %s", self.name, e, response.keys())
            return []

    @property
    def name(self):
        return "s3_summary"

    @property
    def interval(self):
        return 86400

    def _get_colleague_contents(self):
        return AWSColleagueContents(None, 0)

    def _fetch_raw_content(self, colleague_contents):
        buckets = []
        for bucket in self._buckets:
            bucket_name = bucket['Name']
            response = self._client.get_bucket_location(Bucket=bucket_name)
            try:
                location = response['LocationConstraint']
            except KeyError as e:
                logging.info("%s/%s: KeyError %s; Available are %s", self.name, bucket_name, e,
                             response.keys())
                location = None

            # We can request buckets globally but if a bucket is located in
            # another region we do not get any results
            if location is None or location != self._region:
                continue
            bucket['LocationConstraint'] = location

            try:
                response = self._client.get_bucket_tagging(Bucket=bucket_name)
                tagging = response['TagSet']
            except botocore.exceptions.ClientError as e:
                # If there are no tags attached to a bucket we receive a 'ClientError'
                logging.info("%s/%s: No tags set, %s", self.name, bucket_name, e)
            except KeyError as e:
                logging.info("%s/%s: KeyError %s; Available are %s", self.name, bucket_name, e,
                             response.keys())
            else:
                bucket['Tagging'] = tagging
            buckets.append(bucket)
        return buckets

    def _compute_content(self, raw_content, colleague_contents):
        return AWSComputedContent(raw_content.content, raw_content.cache_timestamp)

    def _create_results(self, computed_content):
        return [AWSSectionResult("", computed_content.content)]


class S3(AWSSectionCloudwatch):
    @property
    def name(self):
        return "s3"

    @property
    def interval(self):
        return 86400

    def _get_colleague_contents(self):
        colleague = self._received_results.get('s3_summary')
        if colleague and colleague.content:
            return AWSColleagueContents([bucket['Name'] for bucket in colleague.content],
                                        colleague.cache_timestamp)
        return AWSColleagueContents([], 0)

    def _get_metrics(self, colleague_contents):
        metrics = []
        idx = 0
        for bucket_name in colleague_contents.content:
            for metric_name, unit, storage_classes in [
                ("BucketSizeBytes", "Bytes", [
                    "StandardStorage",
                    "StandardIAStorage",
                    "ReducedRedundancyStorage",
                ]),
                ("NumberOfObjects", "Count", ["AllStorageTypes"]),
            ]:
                for storage_class in storage_classes:
                    metrics.append({
                        'Id': "%s_%s_%s" % (metric_name.lower(), storage_class.lower(), idx),
                        'Label': bucket_name,
                        'MetricStat': {
                            'Metric': {
                                'Namespace': 'AWS/S3',
                                'MetricName': metric_name,
                                'Dimensions': [{
                                    'Name': "BucketName",
                                    'Value': bucket_name,
                                }, {
                                    'Name': 'StorageType',
                                    'Value': storage_class,
                                }]
                            },
                            'Period': self.period,
                            'Stat': 'Average',
                            'Unit': unit,
                        },
                    })
                    idx += 1
        return metrics

    def _compute_content(self, raw_content, colleague_contents):
        return AWSComputedContent(raw_content.content, raw_content.cache_timestamp)

    def _create_results(self, computed_content):
        return [AWSSectionResult("", computed_content.content)]


#.
#   .--ELB-----------------------------------------------------------------.
#   |                          _____ _     ____                            |
#   |                         | ____| |   | __ )                           |
#   |                         |  _| | |   |  _ \                           |
#   |                         | |___| |___| |_) |                          |
#   |                         |_____|_____|____/                           |
#   |                                                                      |
#   '----------------------------------------------------------------------'


class ELBSummary(AWSSectionGeneric):
    @property
    def name(self):
        return "elb_summary"

    @property
    def interval(self):
        return 300

    def _get_colleague_contents(self):
        return AWSColleagueContents(None, 0)

    def _fetch_raw_content(self, colleague_contents):
        response = self._client.describe_load_balancers()
        try:
            load_balancers = response['LoadBalancerDescriptions']
        except KeyError, e:
            logging.info("%s: KeyError %s; Available are %s", self.name, e, response.keys())
            return []

        for load_balancer in load_balancers:
            load_balancer_name = load_balancer['LoadBalancerName']
            response = self._client.describe_tags(LoadBalancerNames=[load_balancer_name])
            try:
                tag_descrs = response['TagDescriptions']
            except KeyError as e:
                logging.info("%s/%s: KeyError %s; Available are %s", self.name, load_balancer_name,
                             e, response.keys())
            else:
                load_balancer['TagDescriptions'] = tag_descrs
        return load_balancers

    def _compute_content(self, raw_content, colleague_contents):
        content_by_piggyback_hosts = {}
        for load_balancer in raw_content.content:
            content_by_piggyback_hosts.setdefault(load_balancer['LoadBalancerName'],
                                                  []).append(load_balancer)
        return AWSComputedContent(content_by_piggyback_hosts, raw_content.cache_timestamp)

    def _create_results(self, computed_content):
        return [
            AWSSectionResult(piggyback_hostname, rows)
            for piggyback_hostname, rows in computed_content.content.iteritems()
        ]


class ELBHealth(AWSSectionGeneric):
    @property
    def name(self):
        return "elb_health"

    @property
    def interval(self):
        return 300

    def _get_colleague_contents(self):
        colleague = self._received_results.get('elb_summary')
        if colleague and colleague.content:
            return AWSColleagueContents(
                [load_balancer['LoadBalancerName'] for load_balancer in colleague.content],
                colleague.cache_timestamp)
        return AWSColleagueContents([], 0)

    def _fetch_raw_content(self, colleague_contents):
        load_balancers = {}
        for load_balancer_name in colleague_contents.content:
            response = self._client.describe_instance_health(LoadBalancerName=load_balancer_name)
            try:
                states = response['InstanceStates']
            except KeyError as e:
                logging.info("%s/%s: KeyError %s; Available are %s", self.name, load_balancer_name,
                             e, response.keys())
            else:
                load_balancers.setdefault(load_balancer_name, states)
        return load_balancers

    def _compute_content(self, raw_content, colleague_contents):
        return AWSComputedContent(raw_content.content, raw_content.cache_timestamp)

    def _create_results(self, computed_content):
        return [
            AWSSectionResult(piggyback_hostname, rows)
            for piggyback_hostname, rows in computed_content.content.iteritems()
        ]


class ELB(AWSSectionCloudwatch):
    @property
    def name(self):
        return "elb"

    @property
    def interval(self):
        return 300

    def _get_colleague_contents(self):
        colleague = self._received_results.get('elb_summary')
        if colleague and colleague.content:
            return AWSColleagueContents(
                [load_balancer['LoadBalancerName'] for load_balancer in colleague.content],
                colleague.cache_timestamp)
        return AWSColleagueContents([], 0)

    def _get_metrics(self, colleague_contents):
        metrics = []
        idx = 0
        for load_balancer_name in colleague_contents.content:
            for metric_name, stat in [
                    # Load balancer metrics
                ("RequestCount", "Sum"),
                ("SurgeQueueLength", "Maximum"),
                ("SpilloverCount", "Sum"),
                ("HTTPCode_ELB_4XX", "Sum"),
                ("HTTPCode_ELB_5XX", "Sum"),
                    # Backend metrics
                ("HealthyHostCount", "Average"),
                ("UnHealthyHostCount", "Average"),
                ("Latency", "Average"),
                ("HTTPCode_Backend_2XX", "Sum"),
                ("HTTPCode_Backend_3XX", "Sum"),
                ("HTTPCode_Backend_4XX", "Sum"),
                ("HTTPCode_Backend_5XX", "Sum"),
                ("BackendConnectionErrors", "Sum"),
            ]:
                metrics.append({
                    'Id': "%s_%s" % (metric_name.lower(), idx),
                    'Label': load_balancer_name,
                    'MetricStat': {
                        'Metric': {
                            'Namespace': 'AWS/ELB',
                            'MetricName': metric_name,
                            'Dimensions': [{
                                'Name': "LoadBalancerName",
                                'Value': load_balancer_name,
                            }]
                        },
                        'Period': self.period,
                        'Stat': stat,
                    },
                })
                idx += 1
        return metrics

    def _compute_content(self, raw_content, colleague_contents):
        content_by_piggyback_hosts = {}
        for row in raw_content.content:
            content_by_piggyback_hosts.setdefault(row['Label'], []).append(row)
        return AWSComputedContent(content_by_piggyback_hosts, raw_content.cache_timestamp)

    def _create_results(self, computed_content):
        return [
            AWSSectionResult(piggyback_hostname, rows)
            for piggyback_hostname, rows in computed_content.content.iteritems()
        ]


#.
#   .--EBS-----------------------------------------------------------------.
#   |                          _____ ____ ____                             |
#   |                         | ____| __ ) ___|                            |
#   |                         |  _| |  _ \___ \                            |
#   |                         | |___| |_) |__) |                           |
#   |                         |_____|____/____/                            |
#   |                                                                      |
#   '----------------------------------------------------------------------'

# EBS are attached to EC2 instances. Thus we put the content to related EC2
# instance as piggyback host.


class EBSSummary(AWSSectionGeneric):
    @property
    def name(self):
        return "ebs_summary"

    @property
    def interval(self):
        return 300

    def _get_colleague_contents(self):
        colleague = self._received_results.get('ec2_summary')
        if colleague and colleague.content:
            return AWSColleagueContents(
                colleague.content.get('mapping', {}), colleague.cache_timestamp)
        return AWSColleagueContents({}, 0)

    def _fetch_raw_content(self, colleague_contents):
        response = self._client.describe_volumes()
        try:
            volumes = {r['VolumeId']: r for r in response['Volumes']}
        except KeyError as e:
            logging.info("%s: KeyError %s; Available are %s", self.name, e, response.keys())
            volumes = {}

        response = self._client.describe_volume_status()
        try:
            volume_states = {r['VolumeId']: r for r in response['VolumeStatuses']}
        except KeyError as e:
            logging.info("%s: KeyError %s; Available are %s", self.name, e, response.keys())
            volume_states = {}
        return volumes, volume_states

    def _compute_content(self, raw_content, colleague_contents):
        volumes, volume_states = raw_content.content
        content = []
        for volume_id in set(volumes.keys()).union(set(volume_states.keys())):
            volume = volumes.get(volume_id, {})
            volume.update(volume_states.get(volume_id, {}))
            content.append(volume)

        content_by_piggyback_hosts = {}
        for row in content:
            for attachment in row['Attachments']:
                attachment_id = attachment['InstanceId']
                instance_name = colleague_contents.content.get(attachment_id, attachment_id)
                content_by_piggyback_hosts.setdefault(instance_name, []).append(row)
        return AWSComputedContent(content_by_piggyback_hosts, raw_content.cache_timestamp)

    def _create_results(self, computed_content):
        return [
            AWSSectionResult(piggyback_hostname, rows)
            for piggyback_hostname, rows in computed_content.content.iteritems()
        ]


class EBS(AWSSectionCloudwatch):
    @property
    def name(self):
        return "ebs"

    @property
    def interval(self):
        return 300

    def _get_colleague_contents(self):
        colleague = self._received_results.get('ebs_summary')
        if colleague and colleague.content:
            return AWSColleagueContents([(instance_name, row['VolumeId'], row['VolumeType'])
                                         for instance_name, rows in colleague.content.iteritems()
                                         for row in rows], colleague.cache_timestamp)
        return AWSColleagueContents([], 0)

    def _get_metrics(self, colleague_contents):
        metrics = []
        idx = 0
        for instance_name, volume_name, volume_type in colleague_contents.content:
            for metric_name, unit, volume_types in [
                    # Disk IO
                ("VolumeReadOps", "Count", []),
                ("VolumeWriteOps", "Count", []),
                ("VolumeReadBytes", "Bytes", []),
                ("VolumeWriteBytes", "Bytes", []),
                ("VolumeThroughputPercentage", "Percent", ["io1"]),
                ("VolumeConsumedReadWriteOps", "Count", ["io1"]),
                    # Latency
                ("VolumeTotalReadTime", "Seconds", []),
                ("VolumeTotalWriteTime", "Seconds", []),
                    # Disk activity
                ("VolumeQueueLength", "Count", []),
                ("VolumeIdleTime", "Seconds", []),
                ("BurstBalance", "Percent", ["gp2", "st1", "sc1"]),
                    # Status checks
                ("VolumeStatus", None, []),
                ("IOPerformance", None, ["io1"]),
            ]:
                if volume_types and volume_type not in volume_types:
                    continue
                metric = {
                    'Id': "%s_%s" % (metric_name.lower(), idx),
                    'Label': instance_name,
                    'MetricStat': {
                        'Metric': {
                            'Namespace': 'AWS/EBS',
                            'MetricName': metric_name,
                            'Dimensions': [{
                                'Name': "VolumeID",
                                'Value': volume_name,
                            }]
                        },
                        'Period': self.period,
                        'Stat': 'Average',
                    },
                }
                if unit:
                    metric['MetricStat']['Unit'] = unit
                metrics.append(metric)
                idx += 1
        return metrics

    def _compute_content(self, raw_content, colleague_contents):
        content_by_piggyback_hosts = {}
        for row in raw_content.content:
            content_by_piggyback_hosts.setdefault(row['Label'], []).append(row)
        return AWSComputedContent(content_by_piggyback_hosts, raw_content.cache_timestamp)

    def _create_results(self, computed_content):
        return [
            AWSSectionResult(piggyback_hostname, rows)
            for piggyback_hostname, rows in computed_content.content.iteritems()
        ]


#.
#   .--glacier-------------------------------------------------------------.
#   |                         _            _                               |
#   |                    __ _| | __ _  ___(_) ___ _ __                     |
#   |                   / _` | |/ _` |/ __| |/ _ \ '__|                    |
#   |                  | (_| | | (_| | (__| |  __/ |                       |
#   |                   \__, |_|\__,_|\___|_|\___|_|                       |
#   |                   |___/                                              |
#   '----------------------------------------------------------------------'

#class Glacier(AWSSectionGeneric):
#    @property
#    def name(self):
#        return "glacier"
#
#    @property
#    def interval(self):
#        return 300
#
#    def _fetch_raw_content(self, colleague_contents):
#        response = self._client.list_vaults()
#        try:
#            vaults = {r['VaultName']: r for r in response['VaultList']}
#        except KeyError as e:
#            logging.info("%s: KeyError %s; Available are %s", self.name, e, response.keys())
#            vaults = {}
#
#        for vault_name, vault in vaults.iteritems():
#            response = self._client.list_jobs(vaultName=vault_name)
#            try:
#                jobs = response['JobList']
#            except KeyError as e:
#                logging.info("%s/%s: KeyError %s; Available are %s", self.name, vault_name, e,
#                             response.keys())
#                jobs = []
#            vault['Jobs'] = jobs
#        return vaults
#
#    def _create_results(self, computed_content):
#        return [AWSSectionResult("", computed_content)]

#.
#   .--cloud trail---------------------------------------------------------.
#   |                 _                 _   _             _ _              |
#   |             ___| | ___  _   _  __| | | |_ _ __ __ _(_) |             |
#   |            / __| |/ _ \| | | |/ _` | | __| '__/ _` | | |             |
#   |           | (__| | (_) | |_| | (_| | | |_| | | (_| | | |             |
#   |            \___|_|\___/ \__,_|\__,_|  \__|_|  \__,_|_|_|             |
#   |                                                                      |
#   '----------------------------------------------------------------------'

# AWS eigener Monitoring service

#class CloudTrail(AWSSectionGeneric):
#    @property
#    def name(self):
#        return "cloud_trail"
#
#    @property
#    def interval(self):
#        return 300
#
#    def _fetch_raw_content(self, colleague_contents):
#        response = self._client.describe_trails()
#        try:
#            trails = {trail['Name']: trail for trail in response['trailList']}
#        except KeyError as e:
#            logging.info("%s: KeyError %s; Available are %s", self.name, e, response.keys())
#            trails = {}
#
#        for trail_name, trail in trails.iteritems():
#            response = self._client.get_trail_status(Name=trail_name)
#            trail['Status'] = response
#        return trails
#
#    def _create_results(self, computed_content):
#        return [AWSSectionResult("", computed_content)]

#.
#   .--sections------------------------------------------------------------.
#   |                               _   _                                  |
#   |                 ___  ___  ___| |_(_) ___  _ __  ___                  |
#   |                / __|/ _ \/ __| __| |/ _ \| '_ \/ __|                 |
#   |                \__ \  __/ (__| |_| | (_) | | | \__ \                 |
#   |                |___/\___|\___|\__|_|\___/|_| |_|___/                 |
#   |                                                                      |
#   '----------------------------------------------------------------------'


class AWSSections(object):
    __metaclass__ = abc.ABCMeta

    def __init__(self, hostname, region, session, services):
        self._hostname = hostname
        self._region = region
        self._session = session
        self._services = services
        self._sections = []

    @abc.abstractmethod
    def init_sections(self, **kwargs):
        pass

    def _init_client(self, client_key):
        try:
            return self._session.client(client_key)
        except (ValueError, botocore.exceptions.ClientError,
                botocore.exceptions.UnknownServiceError) as e:
            # If region name is not valid we get a ValueError
            # but not in all cases, eg.:
            # 1. 'eu-central-' raises a ValueError
            # 2. 'foobar' does not raise a ValueError
            # In the second case we get an exception raised by botocore
            # during we execute an operation, eg. cloudwatch.get_metrics(**kwargs):
            # - botocore.exceptions.EndpointConnectionError
            logging.info(e)

    def run(self, use_cache=True):
        exceptions = []
        results = {}
        for section in self._sections:
            try:
                section_result = section.run(use_cache=use_cache)
            except Exception as e:
                logging.info(e)
                exceptions.append(e)
            else:
                results.setdefault((section.name, section_result.cache_timestamp, section.interval),
                                   section_result.results)

        self._write_exceptions(exceptions)
        self._write_section_results(results)

    def _write_exceptions(self, exceptions):
        sys.stdout.write("<<<aws_exceptions>>>\n")
        if exceptions:
            out = "\n".join([e.message for e in exceptions])
        else:
            out = "No exceptions"
        sys.stdout.write("%s: %s\n" % (self.__class__.__name__, out))

    def _write_section_results(self, results):
        if not results:
            logging.info("No results or cached data")
            return

        for (section_name, cache_timestamp, section_interval), result in results.iteritems():
            if not result:
                logging.info("No results of %s", section_name)
                continue

            if not isinstance(result, list):
                logging.info("Section result must be formatted as a list of 'AWSSectionResult's")
                continue

            cached_suffix = ""
            if section_interval > 60:
                cached_suffix = ":cached(%s,%s)" % (int(cache_timestamp), section_interval + 60)

            if any([r.content for r in result]):
                self._write_section_result(section_name, cached_suffix, result)

    def _write_section_result(self, section_name, cached_suffix, result):
        section_header = "<<<aws_%s%s>>>\n" % (section_name, cached_suffix)
        for row in result:
            write_piggyback_header = row.piggyback_hostname\
                                     and row.piggyback_hostname != self._hostname
            if write_piggyback_header:
                sys.stdout.write("<<<<%s>>>>\n" % row.piggyback_hostname)
            sys.stdout.write(section_header)
            sys.stdout.write("%s\n" % json.dumps(row.content, default=_datetime_converter))
            if write_piggyback_header:
                sys.stdout.write("<<<<>>>>\n")


class AWSSectionsUSEast(AWSSections):
    """
    Some clients like CostExplorer only work with US East region:
    https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/ce-api.html
    """

    def init_sections(self, **kwargs):
        #---clients---------------------------------------------------------
        ce_client = self._init_client('ce')

        #---distributors----------------------------------------------------

        #---sections with distributors--------------------------------------

        #---sections--------------------------------------------------------
        ce = CostsAndUsage(ce_client, self._hostname, self._region)

        #---register sections to distributors-------------------------------

        #---register sections for execution---------------------------------
        if 'ce' in self._services:
            self._sections.append(ce)


class AWSSectionsGeneric(AWSSections):
    def init_sections(self, **kwargs):
        #---clients---------------------------------------------------------
        ec2_client = self._init_client('ec2')
        elb_client = self._init_client('elb')
        s3_client = self._init_client('s3')
        cloudwatch_client = self._init_client('cloudwatch')

        #---distributors----------------------------------------------------
        security_groups_distributor = ResultDistributor()
        ec2_summary_distributor = ResultDistributor()
        elb_summary_distributor = ResultDistributor()
        ebs_summary_distributor = ResultDistributor()
        s3_summary_distributor = ResultDistributor()

        #---sections with distributors--------------------------------------
        security_groups = SecurityGroups(ec2_client, self._hostname, self._region,
                                         security_groups_distributor)
        ec2_summary = EC2Summary(ec2_client, self._hostname, self._region, ec2_summary_distributor)
        ebs_summary = EBSSummary(ec2_client, self._hostname, self._region, ebs_summary_distributor)
        elb_summary = ELBSummary(elb_client, self._hostname, self._region, elb_summary_distributor)
        s3_summary = S3Summary(
            s3_client,
            self._hostname,
            self._region,
            distributor=s3_summary_distributor,
            buckets=kwargs.get('buckets'))

        #---sections--------------------------------------------------------
        elb_health = ELBHealth(elb_client, self._hostname, self._region)
        ec2 = EC2(cloudwatch_client, self._hostname, self._region)
        ebs = EBS(cloudwatch_client, self._hostname, self._region)
        elb = ELB(cloudwatch_client, self._hostname, self._region)
        s3 = S3(cloudwatch_client, self._hostname, self._region)

        #---register sections to distributors-------------------------------
        security_groups_distributor.add(ec2_summary)
        ec2_summary_distributor.add(ec2)
        ec2_summary_distributor.add(ebs_summary)
        ec2_summary_distributor.add(ebs)
        ebs_summary_distributor.add(ebs)
        elb_summary_distributor.add(elb_health)
        elb_summary_distributor.add(elb)
        s3_summary_distributor.add(s3)

        #---register sections for execution---------------------------------
        # Dependencies: First append sections which distribute their results:
        # -- security_groups
        #    |
        #    |--ec2_summary ('ec2')
        #       |
        #       |-- ec2 ('ec2')
        #       |
        #       |-- ebs_summary ('ec2', 'ebs')
        #       |       |
        #       |       |-- ebs ('ec2', 'ebs')
        #       |
        #       |-- ebs ('ec2')
        #
        # -- elb_summary
        #    |
        #    |-- elb_health
        #    |
        #    |-- elb
        if 'ec2' in self._services:
            self._sections.append(security_groups)
            self._sections.append(ec2_summary)
            self._sections.append(ec2)

        if 'ebs' in self._services:
            self._sections.append(ebs_summary)
            self._sections.append(ebs)

        if 'elb' in self._services:
            self._sections.append(elb_summary)
            self._sections.append(elb_health)
            self._sections.append(elb)

        if 's3' in self._services:
            self._sections.append(s3_summary)
            self._sections.append(s3)


#.
#   .--main----------------------------------------------------------------.
#   |                                       _                              |
#   |                       _ __ ___   __ _(_)_ __                         |
#   |                      | '_ ` _ \ / _` | | '_ \                        |
#   |                      | | | | | | (_| | | | | |                       |
#   |                      |_| |_| |_|\__,_|_|_| |_|                       |
#   |                                                                      |
#   '----------------------------------------------------------------------'

AWSRegions = [
    ("ap-south-1", "Asia Pacific (Mumbai)"),
    ("ap-northeast-3", "Asia Pacific (Osaka-Local)"),
    ("ap-northeast-2", "Asia Pacific (Seoul)"),
    ("ap-southeast-1", "Asia Pacific (Singapore)"),
    ("ap-southeast-2", "Asia Pacific (Sydney)"),
    ("ap-northeast-1", "Asia Pacific (Tokyo)"),
    ("ca-central-1", "Canada (Central)"),
    ("cn-north-1", "China (Beijing)"),
    ("cn-northwest-1", "China (Ningxia)"),
    ("eu-central-1", "EU (Frankfurt)"),
    ("eu-west-1", "EU (Ireland)"),
    ("eu-west-2", "EU (London)"),
    ("eu-west-3", "EU (Paris)"),
    ("eu-north-1", "EU (Stockholm)"),
    ("sa-east-1", "South America (Sao Paulo)"),
    ("us-east-2", "US East (Ohio)"),
    ("us-east-1", "US East (N. Virginia)"),
    ("us-west-1", "US West (N. California)"),
    ("us-west-2", "US West (Oregon)"),
]

AWSServices = [
    ("ce", "Costs and usage"),
    ("ec2", "Elastic Compute Cloud (EC2)"),
    ("ebs", "Elastic Block Storage (EBS)"),
    ("s3", "Simple Storage Service (S3)"),
    ("elb", "Elastic Load Balancing (ELB)"),
]


def parse_arguments(argv):
    parser = argparse.ArgumentParser(
        description=__doc__, formatter_class=argparse.RawTextHelpFormatter)
    parser.add_argument("--debug", action="store_true", help="Raise Python exceptions.")
    parser.add_argument(
        "--aws-access-key-id", required=True, help="The access key for your AWS account.")
    parser.add_argument(
        "--aws-secret-access-key", required=True, help="The secret key for your AWS account.")
    parser.add_argument(
        "--regions",
        nargs='+',
        required=True,
        help="Regions to use:\n%s" % "\n".join(["%-15s %s" % e for e in AWSRegions]))
    parser.add_argument(
        "--services",
        nargs='+',
        required=True,
        help="Services to monitor:\n%s" % "\n".join(["%-15s %s" % e for e in AWSServices]))
    parser.add_argument('--buckets', nargs='+', help="Buckets to monitor")
    parser.add_argument(
        "--no-cache", action="store_true", help="Execute all sections, do not rely on cached data.")
    parser.add_argument("--hostname", required=True)
    return parser.parse_args(argv)


def setup_logging(opt_debug):
    fmt = '%(levelname)s: %(name)s: %(filename)s: %(lineno)s: %(message)s'
    if opt_debug:
        lvl = logging.DEBUG
    else:
        lvl = logging.INFO
    logging.basicConfig(level=lvl, format=fmt)


def create_session(access_key_id, secret_access_key, region):
    return boto3.session.Session(
        aws_access_key_id=access_key_id,
        aws_secret_access_key=secret_access_key,
        region_name=region)


def main(args=None):
    if args is None:
        cmk.utils.password_store.replace_passwords()
        args = sys.argv[1:]

    args = parse_arguments(args)
    setup_logging(args.debug)

    hostname = args.hostname
    access_key_id = args.aws_access_key_id
    secret_access_key = args.aws_secret_access_key
    use_cache = not args.no_cache
    services = args.services

    exceptions = []
    try:
        region_us_east_1 = "us-east-1"
        session = create_session(access_key_id, secret_access_key, region_us_east_1)
        aws_sections_us_east = AWSSectionsUSEast(hostname, region_us_east_1, session, services)
        aws_sections_us_east.init_sections()
        aws_sections_us_east.run(use_cache=use_cache)
    except Exception as e:
        if args.debug:
            raise
        exceptions.append(e)

    for region in args.regions:
        try:
            session = create_session(access_key_id, secret_access_key, region)
            aws_sections = AWSSectionsGeneric(hostname, region, session, services)
            aws_sections.init_sections(buckets=args.buckets)
            aws_sections.run(use_cache=use_cache)
        except Exception as e:
            if args.debug:
                raise
            exceptions.append(e)

    if exceptions:
        for e in exceptions:
            sys.stderr.write("%s\n" % e)
        return 1
    return 0


if __name__ == "__main__":
    sys.exit(main())
