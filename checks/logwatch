#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Copyright (C) 2019 tribe29 GmbH - License: GNU General Public License v2
# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and
# conditions defined in the file COPYING, which is part of this source code package.

import typing  # no from import allowed in check plugin :-(. fix upon migration

import collections
import getpass
import itertools
import os
import pathlib

import cmk.utils.debug
import cmk.utils.paths

# for now, import migrated stuff. I know this is stupid, this file will vanish!
import cmk.base.plugins.agent_based.utils.logwatch as logwatch
import cmk.base.plugins.agent_based.logwatch_section as logwatch_section
import cmk.base.plugins.agent_based.logwatch as logwatch_plugin
import cmk.base.plugins.agent_based.utils.eval_regex as eval_regex

ItemData = logwatch.ItemData

NodeData = logwatch.Section

SectionLogwatch = typing.Dict[typing.Optional[str], NodeData]

Parameters = typing.Dict[str, typing.Any]

# Configuration variables in main.mk needed during the actual check
logwatch_dir = cmk.utils.paths.var_dir + '/logwatch'
logwatch_rules = []
logwatch_max_filesize = 500000  # do not save more than 500k of message (configurable)
logwatch_service_output = "default"
logwatch_groups = []

# Deprecated option since 1.6. cmk.base creates a config warning when finding rules
# for this ruleset. Can be dropped with 1.7.
logwatch_patterns = {}

# Variables embedded in precompiled checks
check_config_variables += ["logwatch_dir", "logwatch_max_filesize", "logwatch_service_output"]

logwatch_spool_dir = cmk.utils.paths.var_dir + '/logwatch_spool'

#   .--General-------------------------------------------------------------.
#   |                   ____                           _                   |
#   |                  / ___| ___ _ __   ___ _ __ __ _| |                  |
#   |                 | |  _ / _ \ '_ \ / _ \ '__/ _` | |                  |
#   |                 | |_| |  __/ | | |  __/ | | (_| | |                  |
#   |                  \____|\___|_| |_|\___|_|  \__,_|_|                  |
#   |                                                                      |
#   +----------------------------------------------------------------------+
#   |  General functions, for normal and forwarding logwatch check         |
#   '----------------------------------------------------------------------'


def parse_logwatch(info) -> SectionLogwatch:

    return {
        node: logwatch_section.parse_logwatch([line[1:] for line in node_data])
        for node, node_data in itertools.groupby(info, key=lambda x: x[0])
    }


def _logwatch_is_cache_new(last_run: float, node: typing.Optional[str]) -> bool:
    if node is None:
        return True

    path = "%s/%s" % (cmk.utils.paths.tcp_cache_dir, node)
    if not os.path.exists(path):
        raise MKGeneralException("cache not found: %s" % path)

    return os.stat(path).st_mtime > last_run


# New rule-stule logwatch_rules in WATO friendly consistent rule notation:
#
# logwatch_rules = [
#   ( [ PATTERNS ], ALL_HOSTS, [ "Application", "System" ] ),
# ]
# All [ PATTERNS ] of matching rules will be concatenated in order of
# appearance.
#
# PATTERN is a list like:
# [ ( 'O',      ".*ssh.*" ),          # Make informational (OK) messages from these
#   ( (10, 20), "login"   ),          # Warning at 10 messages, Critical at 20
#   ( 'C',      "bad"     ),          # Always critical
#   ( 'W',      "not entirely bad" ), # Always warning
# ]
#


def inventory_logwatch_single(parsed: SectionLogwatch):
    discoverable_items = logwatch.discoverable_items(*parsed.values())
    forward_settings = host_extra_conf(host_name(), get_checkgroup_parameters('logwatch_ec', []))
    not_forwarded_logs = logwatch.select_forwarded(
        discoverable_items,
        forward_settings,
        invert=True,
    )
    inventory_groups = host_extra_conf(host_name(), logwatch_groups)

    for logfile in not_forwarded_logs:
        if not any(
                logwatch_groups_of_logfile(group_patterns, logfile)
                for group_patterns in inventory_groups):
            yield logfile, {}  # file was in no group


def inventory_logwatch_groups(parsed: SectionLogwatch):
    discoverable_items = logwatch.discoverable_items(*parsed.values())
    forward_settings = host_extra_conf(host_name(), get_checkgroup_parameters('logwatch_ec', []))
    not_forwarded_logs = logwatch.select_forwarded(
        discoverable_items,
        forward_settings,
        invert=True,
    )
    inventory_groups = host_extra_conf(host_name(), logwatch_groups)
    inventory = {}

    for logfile in not_forwarded_logs:
        for group_patterns in inventory_groups:
            newly_found = logwatch_groups_of_logfile(group_patterns, logfile)
            for group_name, pattern_set in newly_found.items():
                inventory.setdefault(group_name, set()).update(pattern_set)

    for group_name, patterns in inventory.items():
        params = {"group_patterns": sorted(patterns)}
        yield group_name, params


#.
#   .--Logwatch------------------------------------------------------------.
#   |              _                           _       _                   |
#   |             | | ___   __ ___      ____ _| |_ ___| |__                |
#   |             | |/ _ \ / _` \ \ /\ / / _` | __/ __| '_ \               |
#   |             | | (_) | (_| |\ V  V / (_| | || (__| | | |              |
#   |             |_|\___/ \__, | \_/\_/ \__,_|\__\___|_| |_|              |
#   |                      |___/                                           |
#   +----------------------------------------------------------------------+
#   |  Normal logwatch check                                               |
#   '----------------------------------------------------------------------'


# In case of a precompiled check, params contains the precompiled
# logwatch patterns for the logfile we deal with. If using check_mk
# without precompiled checks, the params must be None an will be
# ignored.
def check_logwatch(item, params, parsed: SectionLogwatch):
    for result in logwatch.errors(parsed):
        yield int(result.state), result.summary

    params = logwatch_plugin.compile_params(params or [])

    last_run = get_item_state("last_run", 0)
    set_item_state("last_run", time.time())

    loglines = []
    for node, node_data in parsed.items():
        item_data: ItemData = node_data["logfiles"].get(item, {"attr": "missing", "lines": []})
        if _logwatch_is_cache_new(last_run, node):
            loglines.extend(item_data['lines'])

    found = item in logwatch.discoverable_items(*parsed.values())
    yield check_logwatch_generic(item, params, loglines, found)


check_info['logwatch'] = {
    'parse_function': parse_logwatch,
    'inventory_function': inventory_logwatch_single,
    'check_function': check_logwatch,
    'service_description': "Log %s",
    'node_info': True,
    'group': 'logwatch_rules',
}

#.
#   .--logwatch.groups-----------------------------------------------------.
#   |              _                                                       |
#   |             | |_      ____ _ _ __ ___  _   _ _ __  ___               |
#   |             | \ \ /\ / / _` | '__/ _ \| | | | '_ \/ __|              |
#   |             | |\ V  V / (_| | | | (_) | |_| | |_) \__ \              |
#   |             |_| \_/\_(_)__, |_|  \___/ \__,_| .__/|___/              |
#   |                        |___/                |_|                      |
#   '----------------------------------------------------------------------'


def _logwatch_instantiate_matched(match, group_name, inclusion):
    num_perc_s = group_name.count("%s")
    matches = [g or "" for g in match.groups()]

    if len(matches) < num_perc_s:
        raise MKGeneralException(
            "Invalid entry in inventory_logwatch_groups: "
            "group name '%s' contains %d times '%%s', but regular expression "
            "'%s' contains only %d subexpression(s)." % \
            (group_name, num_perc_s, inclusion, len(matches)))

    if not matches:
        return group_name, inclusion

    for num, group in enumerate(matches):
        inclusion = eval_regex.instantiate_regex_pattern_once(inclusion, group)
        group_name = group_name.replace("%%%d" % (num + 1), group)
    return group_name % tuple(matches[:num_perc_s]), inclusion


def logwatch_groups_of_logfile(group_patterns, filename):
    found_these_groups = {}
    for group_name, (inclusion, exclusion) in group_patterns:
        inclusion_is_regex = inclusion.startswith("~")
        exclusion_is_regex = exclusion.startswith("~")

        if inclusion_is_regex:
            reg = regex(inclusion[1:])
            incl_match = reg.match(filename)
            if incl_match:
                group_name, inclusion = _logwatch_instantiate_matched(incl_match, group_name,
                                                                      inclusion)
        else:
            incl_match = fnmatch.fnmatch(filename, inclusion)

        if exclusion_is_regex:
            reg = regex(exclusion[1:])
            excl_match = reg.match(filename)
        else:
            excl_match = fnmatch.fnmatch(filename, exclusion)

        # NOTE: exclusion wins.
        if incl_match and not excl_match:
            found_these_groups.setdefault(group_name, set())
            found_these_groups[group_name].add((inclusion, exclusion))

    return found_these_groups


def _logwatch_match_group_patterns(logfile_name, inclusion, exclusion):

    # NOTE: in the grouped sap and fileinfo checks, the *exclusion* pattern wins.
    inclusion_is_regex = inclusion.startswith("~")
    if inclusion_is_regex:
        incl_match = regex(inclusion[1:]).match(logfile_name)
    else:
        incl_match = fnmatch.fnmatch(logfile_name, inclusion)
    if not incl_match:
        return False

    exclusion_is_regex = exclusion.startswith("~")
    if exclusion_is_regex:
        excl_match = regex(exclusion[1:]).match(logfile_name)
    else:
        excl_match = fnmatch.fnmatch(logfile_name, exclusion)
    return not excl_match


def check_logwatch_groups(item, params, parsed: SectionLogwatch):
    for result in logwatch.errors(parsed):
        yield int(result.state), result.summary

    params = logwatch_plugin.compile_params(params or [])

    group_patterns = set(params['group_patterns'])

    loglines = []
    for node_name, node_data in parsed.items():
        # node info ignored (only used in regular logwatch check)
        for logfile_name, item_data in node_data['logfiles'].items():
            for inclusion, exclusion in group_patterns:
                if _logwatch_match_group_patterns(logfile_name, inclusion, exclusion):
                    loglines.extend(item_data['lines'])
                break

    yield check_logwatch_generic(item, params, loglines, True)


check_info['logwatch.groups'] = {
    'check_function': check_logwatch_groups,
    'inventory_function': inventory_logwatch_groups,
    'service_description': "Log %s",
    'node_info': True,
    'group': 'logwatch_rules',
}

#.


# truncate a file near the specified offset while keeping lines intact
def truncate_by_line(file_path, offset):
    with file_path.open('r+') as handle:
        handle.seek(offset)
        handle.readline()  # ensures we don't cut inside a line
        handle.truncate()


class LogwatchBlock(object):

    CHAR_TO_STATE = {"O": 0, "W": 1, "u": 1, "C": 2}
    STATE_TO_STR = {0: "OK", 1: "WARN"}

    def __init__(self, header, patterns):
        self._timestamp = header.strip("<>").rsplit(None, 1)[0]
        self.worst = -1
        self.lines = []
        self.last_worst_line = ''
        self.counts: typing.Counter[int] = typing.Counter()  # matches of a certain pattern
        self.states_counter = collections.Counter()  # lines with a certain state
        self._patterns = patterns or {}

    def finalize(self):
        state_str = LogwatchBlock.STATE_TO_STR.get(self.worst, "CRIT")
        header = u"<<<%s %s>>>\n" % (self._timestamp, state_str)
        return [header] + self.lines

    def add_line(self, line, skip_reclassification):

        try:
            level, text = line.split(None, 1)
        except ValueError:
            level, text = line.strip(), ""

        if not skip_reclassification:
            level = logwatch.reclassify(self.counts, self._patterns, text, level)

        state = LogwatchBlock.CHAR_TO_STATE.get(level, -1)
        self.worst = max(state, self.worst)

        # Save the last worst line of this block
        if max(state, 0) == self.worst:
            self.last_worst_line = text

        # Count the number of lines by state
        if level != '.':
            self.states_counter[level] += 1

        if not skip_reclassification and level != "I":
            self.lines.append(u"%s %s\n" % (level, text))


class LogwatchBlockCollector(object):
    def __init__(self):
        self.worst = 0
        self.last_worst_line = ""
        self._output_lines = []
        self._states_counter = collections.Counter()

    @property
    def size(self):
        return sum(len(line.encode('utf-8')) for line in self._output_lines)

    def __call__(self, block):
        if not block or block.worst <= -1:
            return

        self._states_counter += block.states_counter

        self._output_lines.extend(block.finalize())
        if block.worst >= self.worst:
            self.worst = block.worst
            self.last_worst_line = block.last_worst_line

    def clear_lines(self):
        self._output_lines = []

    def get_lines(self):
        return self._output_lines

    def get_count_info(self):
        expanded_levels = {"O": "OK", "W": "WARN", "u": "WARN", "C": "CRIT"}
        count_txt = ("%d %s" % (count, expanded_levels.get(level, "IGN"))
                     for level, count in self._states_counter.items())
        return "%s messages" % ', '.join(count_txt)


def check_logwatch_generic(item, patterns, loglines, found):
    import six
    import hashlib
    logmsg_dir = pathlib.Path(logwatch_dir, host_name())

    logmsg_dir.mkdir(parents=True, exist_ok=True)

    logmsg_file_path = logmsg_dir / item.replace("/", "\\")

    # Logfile (=item) section not found and no local file found. This usually
    # means, that the corresponding logfile also vanished on the target host.
    if not found and not logmsg_file_path.exists():
        return (3, "log not present anymore")

    block_collector = LogwatchBlockCollector()
    current_block = None

    logmsg_file_exists = logmsg_file_path.exists()
    mode = 'r+' if logmsg_file_exists else 'w'
    try:
        logmsg_file_handle = logmsg_file_path.open(mode, encoding='utf-8')
    except IOError as exc:
        raise MKGeneralException("User %s cannot open file for writing: %s" %
                                 (getpass.getuser(), exc))

    # TODO: repr() of a dict may change.
    pattern_hash = hashlib.sha256(repr(patterns).encode()).hexdigest()
    net_lines = 0
    # parse cached log lines
    if logmsg_file_exists:
        # new format contains hash of patterns on the first line so we only reclassify if they
        # changed
        initline = logmsg_file_handle.readline().rstrip('\n')
        if initline.startswith('[[[') and initline.endswith(']]]'):
            old_pattern_hash = initline[3:-3]
            skip_reclassification = old_pattern_hash == pattern_hash
        else:
            logmsg_file_handle.seek(0)
            skip_reclassification = False

        logfile_size = logmsg_file_path.stat().st_size
        if skip_reclassification and logfile_size > logwatch_max_filesize:
            # early out: without reclassification the file wont shrink and if it is already at
            # the maximum size, all input is dropped anyway
            if logfile_size > logwatch_max_filesize * 2:
                # if the file is far too large, truncate it
                truncate_by_line(logmsg_file_path, logwatch_max_filesize)
            return (2, "unacknowledged messages have exceeded max size, "
                    "new messages are dropped (limit %s)" %
                    get_bytes_human_readable(logwatch_max_filesize))

        for line in logmsg_file_handle:
            line = line.rstrip('\n')
            # Skip empty lines
            if not line:
                continue
            elif line.startswith('<<<') and line.endswith('>>>'):
                # The section is finished here. Add it to the list of reclassified lines if the
                # state of the block is not "I" -> "ignore"
                block_collector(current_block)
                current_block = LogwatchBlock(line, patterns)
            elif current_block is not None:
                current_block.add_line(line, skip_reclassification)
                net_lines += 1

        # The last section is finished here. Add it to the list of reclassified lines if the
        # state of the block is not "I" -> "ignore"
        block_collector(current_block)

        if skip_reclassification:
            output_size = logmsg_file_handle.tell()
            # when skipping reclassification, output lines contains only headers anyway
            block_collector.clear_lines()
        else:
            output_size = block_collector.size
    else:
        output_size = 0
        skip_reclassification = False

    header = time.strftime("<<<%Y-%m-%d %H:%M:%S UNKNOWN>>>\n")
    output_size += len(header)
    header = six.ensure_str(header)

    # process new input lines - but only when there is some room left in the file
    if output_size < logwatch_max_filesize:
        current_block = LogwatchBlock(header, patterns)
        for line in loglines:
            current_block.add_line(line, False)
            net_lines += 1
            output_size += len(line.encode('utf-8'))
            if output_size >= logwatch_max_filesize:
                break
        block_collector(current_block)

    # when reclassifying, rewrite the whole file, outherwise append
    if not skip_reclassification and block_collector.get_lines():
        logmsg_file_handle.seek(0)
        logmsg_file_handle.truncate()
        logmsg_file_handle.write(u"[[[%s]]]\n" % pattern_hash)

    for line in block_collector.get_lines():
        logmsg_file_handle.write(line)
    # correct output size
    logmsg_file_handle.close()
    if net_lines == 0 and logmsg_file_exists:
        logmsg_file_path.unlink()

    # if logfile has reached maximum size, abort with critical state
    if logmsg_file_path.exists() and logmsg_file_path.stat().st_size > logwatch_max_filesize:
        return (2, "unacknowledged messages have exceeded max size, "
                "new messages are lost (limit %s)" %
                get_bytes_human_readable(logwatch_max_filesize))

    #
    # Render output
    #

    if block_collector.worst <= 0:
        return (0, "no error messages")

    info = block_collector.get_count_info()
    if logwatch_service_output == 'default':
        info += ' (Last worst: "%s")' % block_collector.last_worst_line

    return block_collector.worst, info
